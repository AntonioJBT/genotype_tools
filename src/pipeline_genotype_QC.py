##############################################################################
#
#   Imperial College London
#
#   $Id$
#
#   Copyright (C) 2017 Antonio Berlanga-Taylor
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 3
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline genotype QC
===========================

:Author: Antonio Berlanga-Taylor
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline pre-processes genome-wide genotype data from a microarray (Illumina, Affymetrix) and carries out quality control
processing. It outputs varies plots, tables and a final QC'd file for downstream analysis (GWAS, imputation, etc.).

See the files :file:``pipeline.ini` and :file:`conf.py` to configure with different parameters and reporting.

The pipeline is based on the UK Biobank protocol and other methods. See references. 

It requires CGAT tools (pipelines and scripts).

See notes and further information in:
https://github.com/EpiCompBio/genotype_tools/blob/master/todo_genotype_QC.rst


Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_genotype_QC.py config

Input files
-----------

Genotype file as provided by Illumina, Affymetrix or converted to Plink formats.

Requirements
------------

The pipeline requires various tools to be installed:

Requirements:

Default CGAT setup: 
* CGATPipelines
* cgat tools

* Plink 1.90
* R
.. * samtools >= 1.1

TODO: add versions

Pipeline output
===============

Outputs a genetic marker and individual QC'd file in plink's format plus various descriptive plots and tables in a simple report.


Glossary
========

.. glossary::


Code
====

"""

from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
	'''
    utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


# ---------------------------------------------------
# Specific pipeline tasks, example function:

@transform(("pipeline.ini", "conf.py"),
           regex("(.*)\.(.*)"),
           r"\1.counts")
def countWords(infile, outfile):
    '''count the number of words in the pipeline configuration files.'''

    # the command line statement we want to execute
    statement = '''awk 'BEGIN { printf("word\\tfreq\\n"); } 
    {for (i = 1; i <= NF; i++) freq[$i]++}
    END { for (word in freq) printf "%%s\\t%%d\\n", word, freq[word] }'
    < %(infile)s > %(outfile)s'''

    # execute command in variable statement.
    #
    # The command will be sent to the cluster.  The statement will be
    # interpolated with any options that are defined in in the
    # configuration files or variable that are declared in the calling
    # function.  For example, %(infile)s will we substituted with the
    # contents of the variable "infile".
    P.run()


@transform(countWords,
           suffix(".counts"),
           "_counts.load")
def loadWordCounts(infile, outfile):
    '''load results of word counting into database.'''
    P.load(infile, outfile, "--add-index=word")

#####################################################################

'''
General pipeline steps:
-----

A. Pre-QC steps, GenomeStudio to plink, hg19 liftover, flip strand:

	TO DO: load into pipeline by calling each script or function. Needs a if/else decision (if illumina, convert to xxx, if affy do xxx, else error):

	1. GenomeStudio to plink: by zcall script:
		Script: /groupvol/med-bio/epiUKB/Airwave/coreExome_zcall/zcall_v3.4/convertReportToTPED.py
		Job submission script: /groupvol/med-bio/epiUKB/Airwave/coreExome_genotype/plinkFiles/1_convertReportToTPED.pbs
		Result files: /groupvol/med-bio/epiUKB/Airwave/coreExome_genotype/plinkFiles

	2. Convert from AB allele to illumina TOP/BOT annotation: by plink, using Wrayner's annotation files
		Strand files: /groupvol/med-bio/epiUKB/Airwave/strandFiles
		(from http://www.well.ox.ac.uk/~wrayner/strand/)
		Command: plink --noweb --bfile --update-alleles humancoreexome-12v1-1_a.update_alleles.txt --make-bed --out

	3. Update genome build: hg19/build 37 liftover: by plink, using Wrayner's annotation files, also handles strand
		This includes updating a few attributes (chromosome, position, strand flipping etc)
		Script: http://www.well.ox.ac.uk/~wrayner/strand/update_build.sh
'''

'''
-----

B. Allele frequency report with proportions:
	TO DO write commands into ruffus pipeline, e.g. (see also sh scripts above):
	plink2 --bifle xxx --freq
	cat plink.frq | tr -s ' ' '\t' | cut -f 4 | grep A | wc -l # First column is a tab, so fourth is A1
'''

'''
-----

#. Select homogeneous set of samples to use as set for marker QC (PCA based, with automatic selection using e.g. 'aberrant' R package. This is to avoid artefacts from population structure. Excluded samples are later re-introduced.):
	http://bioinformatics.oxfordjournals.org/content/28/1/134.full.pdf+html
	Use summary statistics, and/or: missingness, ancestry, probe intensity, gender separately:
	TO DO write commands into ruffus pipeline:
		- Merge plates first
		
	TO DO write commands into ruffus pipeline (see scripts above although PCA tool needs changing to FlashPCA probably as older tools won't run on large number of samples):
		- Run PCA against 1000G (or Hapmap) as in UKB appendix 1 (requires using plink MAF >5%, HWE 10^-6, etc for Hapmap or 1000G, then projecting onto these)
		
	TO DO write script to wrap aberrant and make it callable from CLI within pipeline:	
		- aberrant with lambda set to 20 for ancestry PC1 and PC2 as summary stats

'''

'''
-----

#. Per batch marker QC (plink commands; drop failing SNPs from all plates):
	- TO DO write script for this, needs loop calling batch 1 vs all other batches, then batch 2 vs all other batches, etc. with parameters (eg p-values and all the criteria below) can be set by user:
		+ Exclude monomorphic SNPs
		+ Genotype call rate (<98%)
		+ Genotype frequency consistency across batches (Fisher's exact test p-value <10^-12)
		+ Allele frequency consistency versus reference panel (eg Hapmap, Fisher's exact test p-value <10^-12)
		+ Hardy Weinberg equilibrium (p-value <10^-12)

'''

'''
-----

#. Plate/batch PCA (visual outlier detection check)
	TO DO clean up commands from above and plotting script for this (may need substantial re-working with tools that take thousands of samples, check notes/discuss)

'''

'''
-----

#. Plate/batch merge
	TO DO write scripts/commands

'''

'''
-----

#. Visual test of genotype calls in cluster plots (bin by MAF, pick random subset)
	TO DO write scripts for this: Gao has plotted these before and I think has scripts. Obviously can't check thousands of SNPs visually svo either use a random pick (e.g. grab 20 or whatever is plottable) or better grab top 10 highest quality SNPs, bottom 10, 10 failed SNPs, 10 at MAF > 10%, 10 at 1-5%, 10 <1%, etc. The aim is to have some visual sanity check of the raw data for some of the markers.

'''

'''
-----

#. Pooled sample QC (all samples; based on high quality set of markers from above; plink commands):
	TO DO these are plink commands that can be put directly into the ruffus pipeline with a PARAMS config option so user can set different cut-offs (these PARAMS and config file are standard for CGAT pipelines):
     - Run with autosomal SNPs only
     - Heterozygosity (standard deviation > +/- 3) and genotype failure rates per individual (>5%)
     - Relatedness between individuals (IBD cut-off >0.185)
     - Gender mis-identification check

'''

'''
-----

#. VCF check sanity (strand, problematic SNPs, etc.)
TO DO look up tools and insert command into Ruffus, these already exist, plink2 has commands for this.

'''

##################################################################
   
# ---------------------------------------------------
# Generic pipeline tasks
@follows(loadWordCounts)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
